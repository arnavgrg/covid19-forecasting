{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwmmF47oWOky"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5J878y42LkC",
    "outputId": "d6b51969-93a7-4fec-c190-e67c4509895f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from scipy.stats import zscore\n",
    "import time\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Add path to root directory with data\n",
    "sys.path += ['./data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Paths\n",
    "TRAIN_PATH      = sys.path[-1] + '/train.csv'\n",
    "GRAPH_PATH      = sys.path[-1] + '/graph.csv'\n",
    "TEST_PATH       = sys.path[-1] + '/test.csv'\n",
    "MERGED_PATH     = sys.path[-1] + '/merged_raw.csv'\n",
    "SUBMISSION_PATH = sys.path[-1] + '/submission.csv'\n",
    "\n",
    "# Kaggle Baseline Score\n",
    "BASELINE_MAPE = 2.93299\n",
    "\n",
    "# Set to True if you want to merge dataframes from scratch\n",
    "MERGE_DATAFRAMES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_column_values(data_frame, col_name):\n",
    "    '''\n",
    "        View the distribution of data within a particular column\n",
    "        in a given dataframe\n",
    "    '''\n",
    "    return data_frame[col_name].value_counts()\n",
    "\n",
    "\n",
    "def count_NaNs(data_frame, col_name):\n",
    "    '''\n",
    "        Return the number of NaN values in a particular column for \n",
    "        a given dataframe\n",
    "    '''\n",
    "    return data_frame[col_name].isnull().sum()\n",
    "\n",
    "\n",
    "def calculate_NaN_replacement(data_frame, col_name, state, strategy=None):\n",
    "    '''\n",
    "        Helper for fill_NaNs\n",
    "        Different strategies for filling NaN values (default = 0)\n",
    "        - Average: Return mean of the column for the state\n",
    "        - Median: Return the median of the column for the state\n",
    "    '''\n",
    "    if strategy=='mean' or strategy=='average':\n",
    "        return data_frame[data_frame.Province_State == state][col_name].mean()\n",
    "    elif strategy=='median':\n",
    "        return data_frame[data_frame.Province_State == state][col_name].mean()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def calculate_moving_averages(data_frame, state, col_name, n):\n",
    "    '''\n",
    "        Returns a Pandas Series of computed moving averages for a given column\n",
    "        Inputs:\n",
    "            data_frame: merged_df\n",
    "            state: state name\n",
    "            col_name: feature to calculate moving average for\n",
    "            n: number of days we want to calculate the moving average for\n",
    "    '''\n",
    "    # Get last n days of data from dataframe for that particular column\n",
    "    subset = list(data_frame[data_frame.Province_State == state][col_name][-n:])\n",
    "    subset += [0]*26\n",
    "    for i in range(n, len(subset)):\n",
    "        subset[i] = np.array(subset[i-n:i]).mean()\n",
    "    subset = subset[n:]\n",
    "    return subset\n",
    "\n",
    "\n",
    "def one_hot_encode(dataframe, feature):\n",
    "    '''\n",
    "    Returns one hot encoded 2D vector\n",
    "    '''\n",
    "    n, d = dataframe.shape\n",
    "    one_hot_vector = np.zeros((n, len(dataframe[feature].unique())))\n",
    "    # Create mapping\n",
    "    mapping = dict()\n",
    "    for idx, val in enumerate(dataframe[feature].unique()):\n",
    "        mapping[val] = idx\n",
    "    # Create one hot vector\n",
    "    for index, row in dataframe.iterrows():\n",
    "        one_hot_vector[index][mapping[row[feature]]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "\n",
    "def normalize(dataframe, feature):\n",
    "    '''\n",
    "    Takes an ordinal feature and normalizes its range to between [0,1],\n",
    "    where 0 is the min value and 1 corresponds to the max value.\n",
    "    '''\n",
    "    max_ = max(dataframe[feature])\n",
    "    min_ = min(dataframe[feature])\n",
    "    normalized = dataframe[feature].apply(lambda x: (x-min_)/(max_-min_))\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def percentage_error(actual, predicted):\n",
    "    '''\n",
    "        Helper method for MAPE. Returns percentage error between\n",
    "        predicted values and ground truth values.\n",
    "    '''\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    '''\n",
    "        Inputs:\n",
    "            - y_true: ground truth values\n",
    "            - y_pred: predicted values\n",
    "        Returns:\n",
    "            - Mean Absolute Percentage Error (MAPE), a value that lies in the range [0,100]\n",
    "    '''\n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0kn5HJyWK-Q"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dI0Hta75547r"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "graph = pd.read_csv(GRAPH_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM4MhAgEdOkq"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Id row because it has 7100 unique values, the same as the number of rows in the dataset\n",
    "train.drop(['ID'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "513sonaAgn8l"
   },
   "source": [
    "#### Create training data sorted by state and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "rRk_tnS7dRuo",
    "outputId": "f11dec18-6eaf-4e96-a642-9bab4949a5f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>People_Tested</th>\n",
       "      <th>People_Hospitalized</th>\n",
       "      <th>Mortality_Rate</th>\n",
       "      <th>Testing_Rate</th>\n",
       "      <th>Hospitalization_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>04-12-2020</td>\n",
       "      <td>3563</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3470.0</td>\n",
       "      <td>75.988020</td>\n",
       "      <td>21583.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>2.610160</td>\n",
       "      <td>460.300152</td>\n",
       "      <td>12.264945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>04-13-2020</td>\n",
       "      <td>3734</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3635.0</td>\n",
       "      <td>79.634933</td>\n",
       "      <td>29182.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>2.651312</td>\n",
       "      <td>622.363852</td>\n",
       "      <td>12.238886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>04-14-2020</td>\n",
       "      <td>3953</td>\n",
       "      <td>114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3839.0</td>\n",
       "      <td>84.305541</td>\n",
       "      <td>33117.0</td>\n",
       "      <td>493.0</td>\n",
       "      <td>2.883886</td>\n",
       "      <td>706.285508</td>\n",
       "      <td>12.471541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province_State        Date  Confirmed  Deaths  Recovered  Active  \\\n",
       "0        Alabama  04-12-2020       3563      93        NaN  3470.0   \n",
       "1        Alabama  04-13-2020       3734      99        NaN  3635.0   \n",
       "2        Alabama  04-14-2020       3953     114        NaN  3839.0   \n",
       "\n",
       "   Incident_Rate  People_Tested  People_Hospitalized  Mortality_Rate  \\\n",
       "0      75.988020        21583.0                437.0        2.610160   \n",
       "1      79.634933        29182.0                457.0        2.651312   \n",
       "2      84.305541        33117.0                493.0        2.883886   \n",
       "\n",
       "   Testing_Rate  Hospitalization_Rate  \n",
       "0    460.300152             12.264945  \n",
       "1    622.363852             12.238886  \n",
       "2    706.285508             12.471541  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all state names\n",
    "state_names = train['Province_State'].unique()\n",
    "\n",
    "# Group training data by state names\n",
    "group_by_state = train.groupby(train.Province_State)\n",
    "\n",
    "# Create training data sorted by state and date\n",
    "training_grouped_by_state = pd.DataFrame()\n",
    "for state in state_names:\n",
    "    state_specific_df = pd.DataFrame(group_by_state.get_group(state))\n",
    "    training_grouped_by_state = training_grouped_by_state.append(state_specific_df)\n",
    "\n",
    "# Reset index since group by creates incorrect indexes\n",
    "training_grouped_by_state = training_grouped_by_state.reset_index(drop=True)\n",
    "\n",
    "training_grouped_by_state.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count NaN values for the 3 features with NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do decide to proceed with a state specific models, we can use this data to create custom datasets for each state depending on the number of NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Alabama\n",
      "\t-- Recovered (NaNs): 35/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Alaska\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 125/142\n",
      "\t-- Hospitalization Rate (NaNs): 125/142\n",
      "\n",
      "State: Arizona\n",
      "\t-- Recovered (NaNs): 2/142\n",
      "\t-- People Hospitalized (NaNs): 5/142\n",
      "\t-- Hospitalization Rate (NaNs): 5/142\n",
      "\n",
      "State: Arkansas\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 5/142\n",
      "\t-- Hospitalization Rate (NaNs): 5/142\n",
      "\n",
      "State: California\n",
      "\t-- Recovered (NaNs): 137/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Colorado\n",
      "\t-- Recovered (NaNs): 13/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Connecticut\n",
      "\t-- Recovered (NaNs): 18/142\n",
      "\t-- People Hospitalized (NaNs): 7/142\n",
      "\t-- Hospitalization Rate (NaNs): 7/142\n",
      "\n",
      "State: Delaware\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Florida\n",
      "\t-- Recovered (NaNs): 137/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Georgia\n",
      "\t-- Recovered (NaNs): 137/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Hawaii\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Idaho\n",
      "\t-- Recovered (NaNs): 5/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Illinois\n",
      "\t-- Recovered (NaNs): 137/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Indiana\n",
      "\t-- Recovered (NaNs): 44/142\n",
      "\t-- People Hospitalized (NaNs): 28/142\n",
      "\t-- Hospitalization Rate (NaNs): 28/142\n",
      "\n",
      "State: Iowa\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Kansas\n",
      "\t-- Recovered (NaNs): 17/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Kentucky\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Louisiana\n",
      "\t-- Recovered (NaNs): 12/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Maine\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Maryland\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Massachusetts\n",
      "\t-- Recovered (NaNs): 78/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Michigan\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Minnesota\n",
      "\t-- Recovered (NaNs): 1/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Mississippi\n",
      "\t-- Recovered (NaNs): 17/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Missouri\n",
      "\t-- Recovered (NaNs): 137/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Montana\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Nebraska\n",
      "\t-- Recovered (NaNs): 47/142\n",
      "\t-- People Hospitalized (NaNs): 56/142\n",
      "\t-- Hospitalization Rate (NaNs): 56/142\n",
      "\n",
      "State: Nevada\n",
      "\t-- Recovered (NaNs): 17/142\n",
      "\t-- People Hospitalized (NaNs): 141/142\n",
      "\t-- Hospitalization Rate (NaNs): 141/142\n",
      "\n",
      "State: New Hampshire\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: New Jersey\n",
      "\t-- Recovered (NaNs): 16/142\n",
      "\t-- People Hospitalized (NaNs): 32/142\n",
      "\t-- Hospitalization Rate (NaNs): 32/142\n",
      "\n",
      "State: New Mexico\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: New York\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: North Carolina\n",
      "\t-- Recovered (NaNs): 24/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: North Dakota\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Ohio\n",
      "\t-- Recovered (NaNs): 76/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Oklahoma\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Oregon\n",
      "\t-- Recovered (NaNs): 18/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Pennsylvania\n",
      "\t-- Recovered (NaNs): 35/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Rhode Island\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: South Carolina\n",
      "\t-- Recovered (NaNs): 6/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: South Dakota\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Tennessee\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Texas\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Utah\n",
      "\t-- Recovered (NaNs): 1/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Vermont\n",
      "\t-- Recovered (NaNs): 5/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Virginia\n",
      "\t-- Recovered (NaNs): 2/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Washington\n",
      "\t-- Recovered (NaNs): 137/142\n",
      "\t-- People Hospitalized (NaNs): 26/142\n",
      "\t-- Hospitalization Rate (NaNs): 26/142\n",
      "\n",
      "State: West Virginia\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 126/142\n",
      "\t-- Hospitalization Rate (NaNs): 126/142\n",
      "\n",
      "State: Wisconsin\n",
      "\t-- Recovered (NaNs): 18/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n",
      "State: Wyoming\n",
      "\t-- Recovered (NaNs): 0/142\n",
      "\t-- People Hospitalized (NaNs): 4/142\n",
      "\t-- Hospitalization Rate (NaNs): 4/142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_number_of_days = len(graph.columns[2:])\n",
    "for state in state_names:\n",
    "    state_specific_data = training_grouped_by_state[training_grouped_by_state.Province_State == state]\n",
    "    print(\"State: {}\".format(state))\n",
    "    print(\"\\t-- Recovered (NaNs): {}/{}\".format(count_NaNs(state_specific_data, 'Recovered'), total_number_of_days))\n",
    "    print(\"\\t-- People Hospitalized (NaNs): {}/{}\".format(count_NaNs(state_specific_data, 'People_Hospitalized'), total_number_of_days))\n",
    "    print(\"\\t-- Hospitalization Rate (NaNs): {}/{}\".format(count_NaNs(state_specific_data, 'Hospitalization_Rate'), total_number_of_days))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8jvJm1xkH3H"
   },
   "source": [
    "#### Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "w5UQX6KLkLxo",
    "outputId": "48152795-2a4b-49b9-c1e5-ea6493d5b5dc"
   },
   "outputs": [],
   "source": [
    "if MERGE_DATAFRAMES:\n",
    "    # Create a new column for each state in the dataset\n",
    "    for state in state_names:\n",
    "        training_grouped_by_state[state] = 0\n",
    "    print(\"New number of columns:\", training_grouped_by_state.shape[1])\n",
    "    # Get list of all the dates\n",
    "    all_dates = graph.columns[2:]\n",
    "    # Initialize empty dataframe\n",
    "    merged_df = pd.DataFrame()\n",
    "    start = time.time()\n",
    "    # Try and merge these two dataframes together\n",
    "    for target_state in state_names:\n",
    "        # Make sure target states are the same\n",
    "        state_train_data = training_grouped_by_state[training_grouped_by_state.Province_State == target_state]\n",
    "        state_graph_data = graph[graph.target_state == target_state]\n",
    "        # Loop through all the source_states and dates\n",
    "        for source_state in state_names:\n",
    "            for date in all_dates:\n",
    "                row = state_train_data[state_train_data.Date == date]\n",
    "                row_number = row.index[0]\n",
    "                # Update value\n",
    "                state_train_data.at[row_number, source_state] = state_graph_data[state_graph_data.source_state == source_state][date]\n",
    "        # Append data for this target state to the created dataframe\n",
    "        merged_df = merged_df.append(state_train_data)\n",
    "        print(\"Completed: \", target_state)\n",
    "    end = time.time()\n",
    "    print(\"Total merge time: \", end-start)\n",
    "    # Save Merged DataFrame\n",
    "    merged_df.to_csv('./data/merged_raw.csv', index=False)\n",
    "else:\n",
    "    merged_df = pd.read_csv(MERGED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaN values with 0s for that particular column [Done on a per-state basis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/core/series.py:4535: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    }
   ],
   "source": [
    "# CAN IGNORE SET WITH COPY WARNING\n",
    "temp_df = pd.DataFrame()\n",
    "for state in state_names:\n",
    "    # Get mean values for Recovered, People Hospitalized and Hospitalization Rate\n",
    "    mean_recovered = calculate_NaN_replacement(merged_df, 'Recovered', state, None)\n",
    "    mean_hospitalizations = calculate_NaN_replacement(merged_df, 'People_Hospitalized', state, None)\n",
    "    mean_hospitalization_rate = calculate_NaN_replacement(merged_df, 'Hospitalization_Rate', state, None)\n",
    "    # Get Slice\n",
    "    state_df = merged_df[merged_df.Province_State == state]\n",
    "    # Update values\n",
    "    state_df['Recovered'].fillna(mean_recovered, inplace=True)\n",
    "    state_df['People_Hospitalized'].fillna(mean_hospitalizations, inplace=True)\n",
    "    state_df['Hospitalization_Rate'].fillna(mean_hospitalization_rate, inplace=True)\n",
    "    # Add to temp df\n",
    "    temp_df = temp_df.append(state_df)\n",
    "merged_df = temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Break] Calculate Moving Average For Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      " ['Province_State', 'Date', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'Incident_Rate', 'People_Tested', 'People_Hospitalized', 'Mortality_Rate', 'Testing_Rate', 'Hospitalization_Rate', 'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ForecastID</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Date</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Active</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>People_Tested</th>\n",
       "      <th>People_Hospitalized</th>\n",
       "      <th>...</th>\n",
       "      <th>South Dakota</th>\n",
       "      <th>Tennessee</th>\n",
       "      <th>Texas</th>\n",
       "      <th>Utah</th>\n",
       "      <th>Vermont</th>\n",
       "      <th>Virginia</th>\n",
       "      <th>Washington</th>\n",
       "      <th>West Virginia</th>\n",
       "      <th>Wisconsin</th>\n",
       "      <th>Wyoming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>09-01-2020</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>47550.285714</td>\n",
       "      <td>72467.571429</td>\n",
       "      <td>2490.760015</td>\n",
       "      <td>9.621144e+05</td>\n",
       "      <td>5971.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>48.428571</td>\n",
       "      <td>7150.000000</td>\n",
       "      <td>3141.571429</td>\n",
       "      <td>83.571429</td>\n",
       "      <td>22.428571</td>\n",
       "      <td>703.142857</td>\n",
       "      <td>167.857143</td>\n",
       "      <td>159.285714</td>\n",
       "      <td>228.714286</td>\n",
       "      <td>33.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>09-01-2020</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2145.571429</td>\n",
       "      <td>2885.428571</td>\n",
       "      <td>692.760820</td>\n",
       "      <td>3.407939e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.428571</td>\n",
       "      <td>55.571429</td>\n",
       "      <td>301.571429</td>\n",
       "      <td>80.714286</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>50.714286</td>\n",
       "      <td>463.142857</td>\n",
       "      <td>7.285714</td>\n",
       "      <td>70.857143</td>\n",
       "      <td>20.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>09-01-2020</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>30124.857143</td>\n",
       "      <td>165539.571429</td>\n",
       "      <td>2756.200028</td>\n",
       "      <td>1.183481e+06</td>\n",
       "      <td>9166.714286</td>\n",
       "      <td>...</td>\n",
       "      <td>136.571429</td>\n",
       "      <td>403.428571</td>\n",
       "      <td>3026.571429</td>\n",
       "      <td>1249.714286</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>367.857143</td>\n",
       "      <td>1397.285714</td>\n",
       "      <td>35.857143</td>\n",
       "      <td>519.000000</td>\n",
       "      <td>137.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ForecastID Province_State        Date  Confirmed  Deaths     Recovered  \\\n",
       "0           0        Alabama  09-01-2020         -1      -1  47550.285714   \n",
       "1           1         Alaska  09-01-2020         -1      -1   2145.571429   \n",
       "2           2        Arizona  09-01-2020         -1      -1  30124.857143   \n",
       "\n",
       "          Active  Incident_Rate  People_Tested  People_Hospitalized  ...  \\\n",
       "0   72467.571429    2490.760015   9.621144e+05          5971.857143  ...   \n",
       "1    2885.428571     692.760820   3.407939e+05             0.000000  ...   \n",
       "2  165539.571429    2756.200028   1.183481e+06          9166.714286  ...   \n",
       "\n",
       "   South Dakota    Tennessee        Texas         Utah    Vermont    Virginia  \\\n",
       "0     48.428571  7150.000000  3141.571429    83.571429  22.428571  703.142857   \n",
       "1     12.428571    55.571429   301.571429    80.714286   2.142857   50.714286   \n",
       "2    136.571429   403.428571  3026.571429  1249.714286   3.714286  367.857143   \n",
       "\n",
       "    Washington  West Virginia   Wisconsin     Wyoming  \n",
       "0   167.857143     159.285714  228.714286   33.142857  \n",
       "1   463.142857       7.285714   70.857143   20.428571  \n",
       "2  1397.285714      35.857143  519.000000  137.571429  \n",
       "\n",
       "[3 rows x 63 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CAN IGNORE SET WITH COPY WARNING\n",
    "\n",
    "print(\"Columns:\\n\", list(merged_df.columns))\n",
    "\n",
    "# Drop columns that we don't want to calculate moving averages for\n",
    "features = list(merged_df.columns)[4:]\n",
    "\n",
    "temp_df = pd.DataFrame()\n",
    "for state in state_names:\n",
    "    # Get slice of data for that specific state\n",
    "    state_df = test[test.Province_State == state]\n",
    "    # Compute 7 day moving average for each feature\n",
    "    for feature in features:\n",
    "        state_df[feature] = calculate_moving_averages(merged_df, state, feature, 7)\n",
    "    temp_df = temp_df.append(state_df)\n",
    "    \n",
    "temp_df = temp_df.sort_values(by='ForecastID')\n",
    "test = temp_df\n",
    "\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test DataFrame\n",
    "test.to_csv('./data/test_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Resume] Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ttKMNucsbXh"
   },
   "source": [
    "#### Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1NA5RE1SsaII"
   },
   "outputs": [],
   "source": [
    "non_normalized_columns = merged_df[['Province_State', 'Date', 'Confirmed', 'Deaths']]\n",
    "merged_df.drop(['Province_State','Date','Confirmed', 'Deaths'], inplace=True, axis=1)\n",
    "\n",
    "# Perform Z-Score normalization\n",
    "merged_df_cols = merged_df.columns\n",
    "normalized_data = zscore(merged_df)\n",
    "merged_df = pd.DataFrame(data=normalized_data, columns=merged_df_cols)\n",
    "\n",
    "# Reassign Columns\n",
    "merged_df['Province_State'] = non_normalized_columns['Province_State']\n",
    "merged_df['Date'] = non_normalized_columns['Date']\n",
    "merged_df['Confirmed'] = non_normalized_columns['Confirmed']\n",
    "merged_df['Deaths'] = non_normalized_columns['Deaths']\n",
    "\n",
    "# Re-ordering\n",
    "merged_df_cols = merged_df.columns.tolist()\n",
    "cols = merged_df_cols[-4:] + merged_df_cols[:len(merged_df_cols)-4]\n",
    "\n",
    "merged_df = merged_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbvH6Nmc5fx1"
   },
   "source": [
    "#### Converting states to one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JE14WNCX35hU"
   },
   "outputs": [],
   "source": [
    "one_hot_states = one_hot_encode(merged_df, 'Province_State')\n",
    "one_hot_states_df = pd.DataFrame(columns=['State_'+x for x in state_names], data=one_hot_states)\n",
    "merged_df = pd.concat([one_hot_states_df, merged_df], axis=1, sort=False)\n",
    "merged_df.drop(['Province_State'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBoQvvtn5iZ0"
   },
   "source": [
    "#### Create Day and Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "JOzX2If65mht",
    "outputId": "170257d4-60f5-4402-d16f-e04fe27abbec"
   },
   "outputs": [],
   "source": [
    "dates = merged_df['Date'].apply(lambda x: x.split('-')[0:2])\n",
    "merged_df['Month'] = [int(x[0]) for x in dates]\n",
    "merged_df['Day'] = [int(x[1]) for x in dates]\n",
    "\n",
    "# Normalize Day and Month between 0 and 1\n",
    "merged_df['Day'] = normalize(merged_df, 'Day')\n",
    "\n",
    "# Exceptional case where we want the max month to be 12 even though this does not show up in the training data\n",
    "max_ = 12\n",
    "min_ = min(merged_df['Month']) #4\n",
    "merged_df['Month'] = merged_df['Month'].apply(lambda x: (x-min_)/(max_-min_))\n",
    "\n",
    "# Drop the date column\n",
    "merged_df.drop(['Date'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Alabama</th>\n",
       "      <th>State_Alaska</th>\n",
       "      <th>State_Arizona</th>\n",
       "      <th>State_Arkansas</th>\n",
       "      <th>State_California</th>\n",
       "      <th>State_Colorado</th>\n",
       "      <th>State_Connecticut</th>\n",
       "      <th>State_Delaware</th>\n",
       "      <th>State_Florida</th>\n",
       "      <th>State_Georgia</th>\n",
       "      <th>...</th>\n",
       "      <th>Texas</th>\n",
       "      <th>Utah</th>\n",
       "      <th>Vermont</th>\n",
       "      <th>Virginia</th>\n",
       "      <th>Washington</th>\n",
       "      <th>West Virginia</th>\n",
       "      <th>Wisconsin</th>\n",
       "      <th>Wyoming</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144582</td>\n",
       "      <td>-0.148633</td>\n",
       "      <td>-0.156990</td>\n",
       "      <td>-0.150194</td>\n",
       "      <td>-0.149977</td>\n",
       "      <td>-0.160389</td>\n",
       "      <td>-0.149111</td>\n",
       "      <td>-0.158190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143976</td>\n",
       "      <td>-0.148079</td>\n",
       "      <td>-0.157352</td>\n",
       "      <td>-0.149995</td>\n",
       "      <td>-0.149988</td>\n",
       "      <td>-0.160083</td>\n",
       "      <td>-0.149022</td>\n",
       "      <td>-0.157914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144111</td>\n",
       "      <td>-0.148148</td>\n",
       "      <td>-0.157171</td>\n",
       "      <td>-0.150255</td>\n",
       "      <td>-0.150046</td>\n",
       "      <td>-0.160389</td>\n",
       "      <td>-0.148865</td>\n",
       "      <td>-0.156811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143980</td>\n",
       "      <td>-0.148033</td>\n",
       "      <td>-0.157533</td>\n",
       "      <td>-0.150324</td>\n",
       "      <td>-0.149966</td>\n",
       "      <td>-0.160160</td>\n",
       "      <td>-0.149167</td>\n",
       "      <td>-0.157500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144028</td>\n",
       "      <td>-0.147433</td>\n",
       "      <td>-0.158438</td>\n",
       "      <td>-0.150437</td>\n",
       "      <td>-0.149977</td>\n",
       "      <td>-0.160006</td>\n",
       "      <td>-0.149089</td>\n",
       "      <td>-0.156121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145325</td>\n",
       "      <td>-0.129430</td>\n",
       "      <td>-0.156809</td>\n",
       "      <td>-0.152003</td>\n",
       "      <td>-0.145456</td>\n",
       "      <td>-0.160696</td>\n",
       "      <td>-0.147614</td>\n",
       "      <td>7.236277</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7096</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145341</td>\n",
       "      <td>-0.123984</td>\n",
       "      <td>-0.155904</td>\n",
       "      <td>-0.151847</td>\n",
       "      <td>-0.145491</td>\n",
       "      <td>-0.160811</td>\n",
       "      <td>-0.147648</td>\n",
       "      <td>7.231174</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145352</td>\n",
       "      <td>-0.122876</td>\n",
       "      <td>-0.155180</td>\n",
       "      <td>-0.152020</td>\n",
       "      <td>-0.146130</td>\n",
       "      <td>-0.160773</td>\n",
       "      <td>-0.147837</td>\n",
       "      <td>6.418884</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145490</td>\n",
       "      <td>-0.128369</td>\n",
       "      <td>-0.158076</td>\n",
       "      <td>-0.151986</td>\n",
       "      <td>-0.146655</td>\n",
       "      <td>-0.160619</td>\n",
       "      <td>-0.148251</td>\n",
       "      <td>6.144580</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7099</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145515</td>\n",
       "      <td>-0.133539</td>\n",
       "      <td>-0.157352</td>\n",
       "      <td>-0.152029</td>\n",
       "      <td>-0.147568</td>\n",
       "      <td>-0.159508</td>\n",
       "      <td>-0.148195</td>\n",
       "      <td>7.828877</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7100 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      State_Alabama  State_Alaska  State_Arizona  State_Arkansas  \\\n",
       "0               1.0           0.0            0.0             0.0   \n",
       "1               1.0           0.0            0.0             0.0   \n",
       "2               1.0           0.0            0.0             0.0   \n",
       "3               1.0           0.0            0.0             0.0   \n",
       "4               1.0           0.0            0.0             0.0   \n",
       "...             ...           ...            ...             ...   \n",
       "7095            0.0           0.0            0.0             0.0   \n",
       "7096            0.0           0.0            0.0             0.0   \n",
       "7097            0.0           0.0            0.0             0.0   \n",
       "7098            0.0           0.0            0.0             0.0   \n",
       "7099            0.0           0.0            0.0             0.0   \n",
       "\n",
       "      State_California  State_Colorado  State_Connecticut  State_Delaware  \\\n",
       "0                  0.0             0.0                0.0             0.0   \n",
       "1                  0.0             0.0                0.0             0.0   \n",
       "2                  0.0             0.0                0.0             0.0   \n",
       "3                  0.0             0.0                0.0             0.0   \n",
       "4                  0.0             0.0                0.0             0.0   \n",
       "...                ...             ...                ...             ...   \n",
       "7095               0.0             0.0                0.0             0.0   \n",
       "7096               0.0             0.0                0.0             0.0   \n",
       "7097               0.0             0.0                0.0             0.0   \n",
       "7098               0.0             0.0                0.0             0.0   \n",
       "7099               0.0             0.0                0.0             0.0   \n",
       "\n",
       "      State_Florida  State_Georgia  ...     Texas      Utah   Vermont  \\\n",
       "0               0.0            0.0  ... -0.144582 -0.148633 -0.156990   \n",
       "1               0.0            0.0  ... -0.143976 -0.148079 -0.157352   \n",
       "2               0.0            0.0  ... -0.144111 -0.148148 -0.157171   \n",
       "3               0.0            0.0  ... -0.143980 -0.148033 -0.157533   \n",
       "4               0.0            0.0  ... -0.144028 -0.147433 -0.158438   \n",
       "...             ...            ...  ...       ...       ...       ...   \n",
       "7095            0.0            0.0  ... -0.145325 -0.129430 -0.156809   \n",
       "7096            0.0            0.0  ... -0.145341 -0.123984 -0.155904   \n",
       "7097            0.0            0.0  ... -0.145352 -0.122876 -0.155180   \n",
       "7098            0.0            0.0  ... -0.145490 -0.128369 -0.158076   \n",
       "7099            0.0            0.0  ... -0.145515 -0.133539 -0.157352   \n",
       "\n",
       "      Virginia  Washington  West Virginia  Wisconsin   Wyoming  Month  \\\n",
       "0    -0.150194   -0.149977      -0.160389  -0.149111 -0.158190    0.0   \n",
       "1    -0.149995   -0.149988      -0.160083  -0.149022 -0.157914    0.0   \n",
       "2    -0.150255   -0.150046      -0.160389  -0.148865 -0.156811    0.0   \n",
       "3    -0.150324   -0.149966      -0.160160  -0.149167 -0.157500    0.0   \n",
       "4    -0.150437   -0.149977      -0.160006  -0.149089 -0.156121    0.0   \n",
       "...        ...         ...            ...        ...       ...    ...   \n",
       "7095 -0.152003   -0.145456      -0.160696  -0.147614  7.236277    0.5   \n",
       "7096 -0.151847   -0.145491      -0.160811  -0.147648  7.231174    0.5   \n",
       "7097 -0.152020   -0.146130      -0.160773  -0.147837  6.418884    0.5   \n",
       "7098 -0.151986   -0.146655      -0.160619  -0.148251  6.144580    0.5   \n",
       "7099 -0.152029   -0.147568      -0.159508  -0.148195  7.828877    0.5   \n",
       "\n",
       "           Day  \n",
       "0     0.366667  \n",
       "1     0.400000  \n",
       "2     0.433333  \n",
       "3     0.466667  \n",
       "4     0.500000  \n",
       "...        ...  \n",
       "7095  0.866667  \n",
       "7096  0.900000  \n",
       "7097  0.933333  \n",
       "7098  0.966667  \n",
       "7099  1.000000  \n",
       "\n",
       "[7100 rows x 112 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Merged DataFrame\n",
    "merged_df.to_csv('./data/merged_transformed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS145.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
